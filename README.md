## A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations

This repository contains the data and code for our ACL 2023 paper, FacialMMT, a framework that uses facial sequences of real speaker to help multimodal emotion recognition.

![overview.png](utils/overview_FacialMMT.jpg)
- **Facial sequences of real speaker in MELD dataset**: In this part, we provide the data of extracted real speaker's facial sequences using our first-stage approach. 
- **A multimodal facial expression-aware multi-task learning model**: In this part, we provide the source code and pre-trained models for ease of both direct evaluation and training from scratch.

***